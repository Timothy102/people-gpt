{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86de3f25-60ed-4920-989e-51118af099e5",
   "metadata": {},
   "source": [
    "# PartnerHQ People Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478ae952-b8d5-409d-9030-8442e071b2e2",
   "metadata": {},
   "source": [
    "### Smaller Dataset RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eb3ce36-8fe5-4b22-b2b6-974eae44c9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_pdl_id</th>\n",
       "      <th>input_profile</th>\n",
       "      <th>status</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>full_name</th>\n",
       "      <th>first_name</th>\n",
       "      <th>middle_initial</th>\n",
       "      <th>middle_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>sex</th>\n",
       "      <th>...</th>\n",
       "      <th>countries</th>\n",
       "      <th>emails</th>\n",
       "      <th>street_addresses</th>\n",
       "      <th>experience</th>\n",
       "      <th>education</th>\n",
       "      <th>profiles</th>\n",
       "      <th>certifications</th>\n",
       "      <th>languages</th>\n",
       "      <th>id</th>\n",
       "      <th>version_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uhvp8IZ3tjq2vxwrLw7DRA_0000</td>\n",
       "      <td>linkedin.com/in/sophie-l-51217729</td>\n",
       "      <td>200</td>\n",
       "      <td>9.0</td>\n",
       "      <td>sophie l</td>\n",
       "      <td>sophie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "      <td>...</td>\n",
       "      <td>['united states']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'company': {'name': 'shepard design', 'size'...</td>\n",
       "      <td>[{'school': {'name': 'ozenne school of communi...</td>\n",
       "      <td>[{'network': 'linkedin', 'id': '98944286', 'ur...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>uhvp8IZ3tjq2vxwrLw7DRA_0000</td>\n",
       "      <td>{'status': '', 'contains': [], 'previous_versi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ruufu9JGLCBPuyPgUdxY7g_0000</td>\n",
       "      <td>linkedin.com/in/catherine-purdue-b7570a41</td>\n",
       "      <td>200</td>\n",
       "      <td>9.0</td>\n",
       "      <td>catherine purdue</td>\n",
       "      <td>catherine</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>purdue</td>\n",
       "      <td>female</td>\n",
       "      <td>...</td>\n",
       "      <td>['canada', 'united states']</td>\n",
       "      <td>[{'address': 'catherinepurdue@flash.net', 'typ...</td>\n",
       "      <td>[{'name': 'monroeville, pennsylvania, united s...</td>\n",
       "      <td>[{'company': {'name': 'popfaktory studio', 'si...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'network': 'linkedin', 'id': '148073993', 'u...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'name': 'english', 'proficiency': None}]</td>\n",
       "      <td>Ruufu9JGLCBPuyPgUdxY7g_0000</td>\n",
       "      <td>{'status': '', 'contains': [], 'previous_versi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jSYMRCwSBCyEl2LnjbczSg_0000</td>\n",
       "      <td>linkedin.com/in/avery-reid-b1b312175</td>\n",
       "      <td>200</td>\n",
       "      <td>9.0</td>\n",
       "      <td>avery reid</td>\n",
       "      <td>avery</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>reid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['united states']</td>\n",
       "      <td>[{'address': 'avery.reid@dynetics.com', 'type'...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'company': {'name': 'northrop grumman', 'siz...</td>\n",
       "      <td>[{'school': None, 'degrees': [], 'start_date':...</td>\n",
       "      <td>[{'network': 'linkedin', 'id': '696506567', 'u...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>jSYMRCwSBCyEl2LnjbczSg_0000</td>\n",
       "      <td>{'status': '', 'contains': [], 'previous_versi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9yM8lI6LBm3dCl9KMyAGSg_0000</td>\n",
       "      <td>linkedin.com/in/insightfulit</td>\n",
       "      <td>200</td>\n",
       "      <td>9.0</td>\n",
       "      <td>vadim abramov</td>\n",
       "      <td>vadim</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abramov</td>\n",
       "      <td>male</td>\n",
       "      <td>...</td>\n",
       "      <td>['united states']</td>\n",
       "      <td>[{'address': 'vadim@insightfulit.com', 'type':...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'company': {'name': 'mei technology consulti...</td>\n",
       "      <td>[{'school': {'name': 'life university', 'type'...</td>\n",
       "      <td>[{'network': 'linkedin', 'id': '4474836', 'url...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>9yM8lI6LBm3dCl9KMyAGSg_0000</td>\n",
       "      <td>{'status': '', 'contains': [], 'previous_versi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eMm8XbPX8V3PH4eD-khTyw_0000</td>\n",
       "      <td>linkedin.com/in/mandy-malone-58489919</td>\n",
       "      <td>200</td>\n",
       "      <td>9.0</td>\n",
       "      <td>mandy malone</td>\n",
       "      <td>mandy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>malone</td>\n",
       "      <td>female</td>\n",
       "      <td>...</td>\n",
       "      <td>['united states']</td>\n",
       "      <td>[{'address': 'mandy@beautifulandwicked.com', '...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'company': {'name': 'mandys company', 'size'...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'network': 'linkedin', 'id': '64899316', 'ur...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>eMm8XbPX8V3PH4eD-khTyw_0000</td>\n",
       "      <td>{'status': '', 'contains': [], 'previous_versi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  input_pdl_id                              input_profile  \\\n",
       "0  uhvp8IZ3tjq2vxwrLw7DRA_0000          linkedin.com/in/sophie-l-51217729   \n",
       "1  Ruufu9JGLCBPuyPgUdxY7g_0000  linkedin.com/in/catherine-purdue-b7570a41   \n",
       "2  jSYMRCwSBCyEl2LnjbczSg_0000       linkedin.com/in/avery-reid-b1b312175   \n",
       "3  9yM8lI6LBm3dCl9KMyAGSg_0000               linkedin.com/in/insightfulit   \n",
       "4  eMm8XbPX8V3PH4eD-khTyw_0000      linkedin.com/in/mandy-malone-58489919   \n",
       "\n",
       "   status  likelihood         full_name first_name middle_initial middle_name  \\\n",
       "0     200         9.0          sophie l     sophie            NaN         NaN   \n",
       "1     200         9.0  catherine purdue  catherine              m         NaN   \n",
       "2     200         9.0        avery reid      avery            NaN         NaN   \n",
       "3     200         9.0     vadim abramov      vadim            NaN         NaN   \n",
       "4     200         9.0      mandy malone      mandy            NaN         NaN   \n",
       "\n",
       "  last_name     sex  ...                    countries  \\\n",
       "0       NaN  female  ...            ['united states']   \n",
       "1    purdue  female  ...  ['canada', 'united states']   \n",
       "2      reid     NaN  ...            ['united states']   \n",
       "3   abramov    male  ...            ['united states']   \n",
       "4    malone  female  ...            ['united states']   \n",
       "\n",
       "                                              emails  \\\n",
       "0                                                 []   \n",
       "1  [{'address': 'catherinepurdue@flash.net', 'typ...   \n",
       "2  [{'address': 'avery.reid@dynetics.com', 'type'...   \n",
       "3  [{'address': 'vadim@insightfulit.com', 'type':...   \n",
       "4  [{'address': 'mandy@beautifulandwicked.com', '...   \n",
       "\n",
       "                                    street_addresses  \\\n",
       "0                                                 []   \n",
       "1  [{'name': 'monroeville, pennsylvania, united s...   \n",
       "2                                                 []   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "\n",
       "                                          experience  \\\n",
       "0  [{'company': {'name': 'shepard design', 'size'...   \n",
       "1  [{'company': {'name': 'popfaktory studio', 'si...   \n",
       "2  [{'company': {'name': 'northrop grumman', 'siz...   \n",
       "3  [{'company': {'name': 'mei technology consulti...   \n",
       "4  [{'company': {'name': 'mandys company', 'size'...   \n",
       "\n",
       "                                           education  \\\n",
       "0  [{'school': {'name': 'ozenne school of communi...   \n",
       "1                                                 []   \n",
       "2  [{'school': None, 'degrees': [], 'start_date':...   \n",
       "3  [{'school': {'name': 'life university', 'type'...   \n",
       "4                                                 []   \n",
       "\n",
       "                                            profiles certifications  \\\n",
       "0  [{'network': 'linkedin', 'id': '98944286', 'ur...             []   \n",
       "1  [{'network': 'linkedin', 'id': '148073993', 'u...             []   \n",
       "2  [{'network': 'linkedin', 'id': '696506567', 'u...             []   \n",
       "3  [{'network': 'linkedin', 'id': '4474836', 'url...             []   \n",
       "4  [{'network': 'linkedin', 'id': '64899316', 'ur...             []   \n",
       "\n",
       "                                    languages                           id  \\\n",
       "0                                          []  uhvp8IZ3tjq2vxwrLw7DRA_0000   \n",
       "1  [{'name': 'english', 'proficiency': None}]  Ruufu9JGLCBPuyPgUdxY7g_0000   \n",
       "2                                          []  jSYMRCwSBCyEl2LnjbczSg_0000   \n",
       "3                                          []  9yM8lI6LBm3dCl9KMyAGSg_0000   \n",
       "4                                          []  eMm8XbPX8V3PH4eD-khTyw_0000   \n",
       "\n",
       "                                      version_status  \n",
       "0  {'status': '', 'contains': [], 'previous_versi...  \n",
       "1  {'status': '', 'contains': [], 'previous_versi...  \n",
       "2  {'status': '', 'contains': [], 'previous_versi...  \n",
       "3  {'status': '', 'contains': [], 'previous_versi...  \n",
       "4  {'status': '', 'contains': [], 'previous_versi...  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6cc5e8fd-ccf2-4d8a-974d-bcb56c385300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "502"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "12d66d53-0a6d-4cae-8120-f8313df090b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract the last number from the job_company_size string\n",
    "def extract_last_number(size_str):\n",
    "    if isinstance(size_str, str):\n",
    "        try:\n",
    "            return int(size_str.split('-')[-1])\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "# Apply the function to create a new column for the company size\n",
    "data['max_company_size'] = data['job_company_size'].apply(extract_last_number)\n",
    "\n",
    "# Filter the data\n",
    "filtered_data = data[\n",
    "    data['job_company_industry'].str.contains('software', 'fintech') & \n",
    "    (data['max_company_size'] <= 1000)\n",
    "]\n",
    "\n",
    "# Display the filtered data\n",
    "filtered_data[[\"job_title\", \"job_company_name\", \"job_title_role\", \"full_name\", \"location_name\", \"work_email\"]].to_csv(\"Filtered.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca76aafd-635f-4e6c-8b3c-edc7f82eedd7",
   "metadata": {},
   "source": [
    "### Vector Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "736a08b6-19ce-4018-b4db-bcf10bfe6144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim/Documents/code/python/partnerhq/backend/venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/tim/Documents/code/python/partnerhq/backend/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(502, 384)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset (assuming it's a pandas DataFrame)\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Initialize SentenceTransformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to extract relevant fields and prepare text for embedding\n",
    "def prepare_profile_text(profile):\n",
    "    company_industry = profile.get('company_industry', '')\n",
    "    experiences = profile.get('experience', '[]')  # Default to empty list if key not present\n",
    "    inferred_years_experience = profile.get('inferred_years_experience', 0)\n",
    "    \n",
    "    # Handle NaN values\n",
    "    if isinstance(experiences, float) and np.isnan(experiences):\n",
    "        experiences = '[]'\n",
    "    \n",
    "    # Parse experiences JSON string if necessary\n",
    "    if isinstance(experiences, str):\n",
    "        try:\n",
    "            experiences = json.loads(experiences)\n",
    "        except json.JSONDecodeError:\n",
    "            experiences = []  # Set to empty list if JSON is invalid\n",
    "    \n",
    "    # Combine experience descriptions into a single text\n",
    "    experience_texts = []\n",
    "    for exp in experiences:\n",
    "        if isinstance(exp, dict):\n",
    "            company_name = exp.get('company', {}).get('name', '')\n",
    "            role = exp.get('title', '')\n",
    "            description = exp.get('description', '')\n",
    "            experience_texts.append(f\"{role} at {company_name}: {description}\")\n",
    "    \n",
    "    combined_experience_text = \" \".join(experience_texts)\n",
    "    profile_text = f\"Industry: {company_industry}. Experience: {combined_experience_text}. Inferred Years of Experience: {inferred_years_experience}.\"\n",
    "    \n",
    "    return profile_text\n",
    "\n",
    "# Prepare profiles for embedding\n",
    "profiles_texts = []\n",
    "profile_ids = data['id'].astype(str).tolist()  # Convert profile_ids to strings\n",
    "\n",
    "for _, row in data.iterrows():\n",
    "    profile = row.to_dict()\n",
    "    profile_text = prepare_profile_text(profile)\n",
    "    profiles_texts.append(profile_text)\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(profiles_texts)\n",
    "\n",
    "# Print example embedding\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "252bd577-282d-44b4-b840-2c83cebc8d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d40532a-b18e-46b5-9852-5ddd5acd48f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pinecone_index():\n",
    "    pc = Pinecone(api_key=\"c5b0c09e-b66b-45ef-b7cb-d973dfc6f624\", environment=\"eu-central-1\")\n",
    "\n",
    "    # Create an index in Pinecone (if it doesn't already exist)\n",
    "    index_name = \"quickstart\"\n",
    "    if index_name not in pc.list_indexes().names():\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=4096,  # Replace with your model dimensions\n",
    "            metric=\"euclidean\",  # Replace with your model metric\n",
    "            spec=ServerlessSpec(\n",
    "                cloud=\"aws\",\n",
    "                region=\"eu-central-1\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Connect to the index\n",
    "    index = pc.Index(index_name)\n",
    "    index.upsert(vectors)\n",
    "    index_profiles(profile_ids, embeddings)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b5b8d9a-557a-49f0-bd90-2b4f359b7674",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_pinecone_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 19\u001b[0m, in \u001b[0;36mcreate_pinecone_index\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Connect to the index\u001b[39;00m\n\u001b[1;32m     18\u001b[0m index \u001b[38;5;241m=\u001b[39m pc\u001b[38;5;241m.\u001b[39mIndex(index_name)\n\u001b[0;32m---> 19\u001b[0m index\u001b[38;5;241m.\u001b[39mupsert(\u001b[43mvectors\u001b[49m)\n\u001b[1;32m     20\u001b[0m index_profiles(profile_ids, embeddings)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectors' is not defined"
     ]
    }
   ],
   "source": [
    "index = create_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd68a67d-5449-49fc-9548-8add61413beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from Pinecone: name 'index' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Function to search profiles based on a natural language query\n",
    "def search_profiles(query, top_k=10):\n",
    "    # Encode the query using the same model used for profile embeddings\n",
    "    query_embedding = model.encode([query])[0]\n",
    "    \n",
    "    # Convert query embedding to a list of floats\n",
    "    query_embedding = query_embedding.tolist()\n",
    "    \n",
    "    # Perform semantic search in Pinecone\n",
    "    try:\n",
    "        query_result = index.query(vector=[query_embedding], top_k=top_k)\n",
    "    except Exception as e:\n",
    "        print(\"Error from Pinecone:\", e)\n",
    "        return []\n",
    "    \n",
    "    # Parse and return results\n",
    "    results = []\n",
    "    for match in query_result['matches']:\n",
    "        profile_id = match['id']\n",
    "        score = match['score']\n",
    "        results.append({\n",
    "            'profile_id': profile_id,\n",
    "            'score': score\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example query\n",
    "query = \"Find me all software engineers in the bay area that have 5+ years of experience and have worked on a marketplace type product in the past\"\n",
    "results = search_profiles(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fc993ee-1a3b-4342-862c-4060434973b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2aad81-11f2-4faa-b124-1969556aa8f8",
   "metadata": {},
   "source": [
    "## LLM Retrieval + Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d191500-d569-4342-8958-19fae48b2c47",
   "metadata": {},
   "source": [
    "### Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bca9b0c1-b004-4ec1-817c-0ac1b55ec147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaadf74-4bcb-4659-ab57-7525437b2fa1",
   "metadata": {},
   "source": [
    "### From HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef1ca85a-81e0-4dc6-81e2-31195109452f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/tim/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ab033f75a247fd83dded6e2d7bc5e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(token=\"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\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb637cdd-f292-4ba6-b7a2-7727307ef64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5b37ee1-ed17-402b-84c2-a60eaec163e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_MPS_ENABLED\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b196351d-1211-4a37-b9c6-b868f4f039e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ced1faa0-a384-4d10-a051-6cc1a1cc86b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "query = \"Find me all software engineers in the bay area that have 5+ years of experience and have worked on a marketplace type product in the past\"\n",
    "inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=200)\n",
    "input_ids = inputs['input_ids'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50990640-9883-4c45-bb6d-2b9ed6b8b8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  8769,   528,   544,  3930, 22488,   297,   272, 17106,  2698,\n",
       "           369,   506, 28705, 28782, 28806,  1267,   302,  2659,   304,   506,\n",
       "          4198,   356,   264,  2668,  2912,  1212,  2093,   297,   272,  2609]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "472cbab6-9d49-45dd-8e00-fed1ffe84662",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Query: \" + query + \"\\nSearch Results:\\n\"\n",
    "for result in results:\n",
    "    context += f\"Profile ID: {result['profile_id']}, Score: {result['score']}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61c121a4-35ae-4618-9056-f821635e1b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attention_mask(input_ids):\n",
    "    # Extract padding token ID (assuming it's set correctly)\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    return (input_ids != pad_token_id).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a000b4f1-fbe8-4146-885e-0f4c10f3c581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=200) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize the context with padding and truncation\n",
    "inputs = tokenizer(context, return_tensors=\"pt\", padding=True, truncation=True, max_length=200)\n",
    "\n",
    "# Create the attention mask\n",
    "attention_mask = create_attention_mask(inputs['input_ids'])\n",
    "\n",
    "# Generate text with attention mask\n",
    "output = model.generate(input_ids=inputs['input_ids'], attention_mask=attention_mask, max_length=200, max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83e6b0d2-4635-4bb1-b459-339384560e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find me all software engineers in the bay area that have 5+ years of experience and have worked on a marketplace type product in the past\n",
      "{'input_ids': tensor([[    1,  8769,   528,   544,  3930, 22488,   297,   272, 17106,  2698,\n",
      "           369,   506, 28705, 28782, 28806,  1267,   302,  2659,   304,   506,\n",
      "          4198,   356,   264,  2668,  2912,  1212,  2093,   297,   272,  2609]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/code/python/partnerhq/backend/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:267\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'shape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Example query\u001b[39;00m\n\u001b[1;32m     57\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFind me all software engineers in the bay area that have 5+ years of experience and have worked on a marketplace type product in the past\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 58\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43msearch_profiles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Generate and print the response\u001b[39;00m\n\u001b[1;32m     61\u001b[0m response \u001b[38;5;241m=\u001b[39m generate_response(query, results)\n",
      "Cell \u001b[0;32mIn[13], line 21\u001b[0m, in \u001b[0;36msearch_profiles\u001b[0;34m(query, top_k)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch_profiles\u001b[39m(query, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m     20\u001b[0m   \u001b[38;5;66;03m# Encode the query using the Mistral model\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m   query_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mencode_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     23\u001b[0m   \u001b[38;5;66;03m# Perform semantic search in Pinecone\u001b[39;00m\n\u001b[1;32m     24\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m, in \u001b[0;36mencode_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 15\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/Documents/code/python/partnerhq/backend/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code/python/partnerhq/backend/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/code/python/partnerhq/backend/venv/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:1139\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1136\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1139\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1151\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1152\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/Documents/code/python/partnerhq/backend/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code/python/partnerhq/backend/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/code/python/partnerhq/backend/venv/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:937\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 937\u001b[0m     batch_size, seq_length \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    939\u001b[0m     batch_size, seq_length, _ \u001b[38;5;241m=\u001b[39m inputs_embeds\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/Documents/code/python/partnerhq/backend/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:269\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pinecone\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Function to encode text using Mistral model\n",
    "def encode_text(text):\n",
    "  inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "  print(inputs)\n",
    "  with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "  return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "# Function to search profiles based on a natural language query\n",
    "def search_profiles(query, top_k=10):\n",
    "  # Encode the query using the Mistral model\n",
    "  query_embedding = encode_text(query)[0]\n",
    "  \n",
    "  # Perform semantic search in Pinecone\n",
    "  try:\n",
    "    query_result = index.query(vector=query_embedding, top_k=top_k)\n",
    "  except Exception as e:\n",
    "    print(\"Error from Pinecone:\", e)\n",
    "    return []\n",
    "  \n",
    "  # Parse and return results\n",
    "  results = []\n",
    "  for match in query_result['matches']:\n",
    "    profile_id = match['id']\n",
    "    score = match['score']\n",
    "    results.append({\n",
    "      'profile_id': profile_id,\n",
    "      'score': score\n",
    "    })\n",
    "  \n",
    "  return results\n",
    "\n",
    "def generate_response(query, search_results):\n",
    "  context = \"Query: \" + query + \"\\nSearch Results:\\n\"\n",
    "  for result in search_results:\n",
    "    context += f\"Profile ID: {result['profile_id']}, Score: {result['score']}\\n\"\n",
    "  \n",
    "  input_text = context + \"\\nResponse:\"\n",
    "  inputs = tokenizer(input_text, return_tensors=\"pt\")  # Tokenize the text\n",
    "  input_ids = inputs['input_ids']  # Extract input IDs from tokenized output\n",
    "\n",
    "  output = model.generate(input_ids=input_ids, max_length=200)\n",
    "  response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "  \n",
    "  return response\n",
    "\n",
    "# Example query\n",
    "query = \"Find me all software engineers in the bay area that have 5+ years of experience and have worked on a marketplace type product in the past\"\n",
    "results = search_profiles(query)\n",
    "\n",
    "# Generate and print the response\n",
    "response = generate_response(query, results)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e23133-fb28-4ccf-88e3-1ba2ac7185a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Optional - Installing xformers -- a pain in the f* ass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f17d915c-7270-4978-ae68-7b205c55af3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/opt/llvm\n"
     ]
    }
   ],
   "source": [
    "!brew --prefix llvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26cd1f2f-8401-4444-98ee-baa677f8b9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PATH=\"/opt/homebrew/opt/llvm/bin:$PATH\"\n",
    "!export LDFLAGS=\"-L/opt/homebrew/opt/llvm/lib\"\n",
    "!export CPPFLAGS=\"-I/opt/homebrew/opt/llvm/include\"\n",
    "!export CC=/opt/homebrew/opt/llvm/bin/clang\n",
    "!export CXX=/opt/homebrew/opt/llvm/bin/clang++\n",
    "!export LIBRARY_PATH=\"/opt/homebrew/opt/llvm/lib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dd3c29e-9bf7-402d-9b61-d253b8aad43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source ~/.zshrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "221a46a9-faab-4ab5-a40d-3cd830c296ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple clang version 15.0.0 (clang-1500.3.9.4)\n",
      "Target: arm64-apple-darwin23.4.0\n",
      "Thread model: posix\n",
      "InstalledDir: /Library/Developer/CommandLineTools/usr/bin\n",
      "Apple clang version 15.0.0 (clang-1500.3.9.4)\n",
      "Target: arm64-apple-darwin23.4.0\n",
      "Thread model: posix\n",
      "InstalledDir: /Library/Developer/CommandLineTools/usr/bin\n"
     ]
    }
   ],
   "source": [
    "!clang --version\n",
    "!clang++ --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb4858e5-434d-4e52-a41a-ce534faf3cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xformers\n",
      "  Using cached xformers-0.0.26.post1.tar.gz (4.1 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=2.1 in /Users/tim/Documents/code/python/partnerhq/venv/lib/python3.12/site-packages (from xformers) (2.3.0)\n",
      "Requirement already satisfied: numpy in /Users/tim/Documents/code/python/partnerhq/venv/lib/python3.12/site-packages (from xformers) (1.26.4)\n",
      "Requirement already satisfied: filelock in /Users/tim/Documents/code/python/partnerhq/venv/lib/python3.12/site-packages (from torch>=2.1->xformers) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/tim/Documents/code/python/partnerhq/venv/lib/python3.12/site-packages (from torch>=2.1->xformers) (4.12.0)\n",
      "Requirement already satisfied: sympy in /Users/tim/Documents/code/python/partnerhq/venv/lib/python3.12/site-packages (from torch>=2.1->xformers) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/tim/Documents/code/python/partnerhq/venv/lib/python3.12/site-packages (from torch>=2.1->xformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/tim/Documents/code/python/partnerhq/venv/lib/python3.12/site-packages (from torch>=2.1->xformers) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/tim/Documents/code/python/partnerhq/venv/lib/python3.12/site-packages (from torch>=2.1->xformers) (2024.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tim/Documents/code/python/partnerhq/venv/lib/python3.12/site-packages (from jinja2->torch>=2.1->xformers) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/tim/Documents/code/python/partnerhq/venv/lib/python3.12/site-packages (from sympy->torch>=2.1->xformers) (1.3.0)\n",
      "Building wheels for collected packages: xformers\n",
      "  Building wheel for xformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for xformers: filename=xformers-0.0.26.post1-cp312-cp312-macosx_14_0_arm64.whl size=622919 sha256=294f4c456c1a8690cfc76fe506a093a870cc19162e88f35d045187ec0b3183e2\n",
      "  Stored in directory: /Users/tim/Library/Caches/pip/wheels/c4/a9/4f/803f19d92c4fc04c2bf65b0ea573a6672ccec24add7c905a60\n",
      "Successfully built xformers\n",
      "Installing collected packages: xformers\n",
      "Successfully installed xformers-0.0.26.post1\n"
     ]
    }
   ],
   "source": [
    "!CXX=/opt/homebrew/opt/llvm/bin/clang++ CC=/opt/homebrew/opt/llvm/bin/clang pip install xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c1ab47-c602-40be-a0fe-81e4d5b1ac78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
